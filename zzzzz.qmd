---
title: "아디다스, 나이키 기사 비교분석"
format: html
author: 언론홍보학과 2019102107 김지후
title-block-style: default
title-block-banner: "#6E6E6E"
date: 2022-10-12
code-fold: true
code-tools: true
---

## 빅카인즈에서 데이터셋을 추출해 두 기업간 비교분석을 실시함

### 1. 필요 패키지 설치

### 2. 각 공사 키워드별 상위단어 분석

### 3. 각 공사 키워드별 긍정, 부정단어 분석


### 1. 필요 패키지 설치


```{r}
#| warning: false

install.packages("readxl",repos = "http://cran.us.r-project.org")
install.packages("tidyverse",repos = "http://cran.us.r-project.org")
install.packages("tidytext",repos = "http://cran.us.r-project.org")
install.packages("kableExtra",repos = "http://cran.us.r-project.org")
install.packages("wordcloud",repos = "http://cran.us.r-project.org")
install.packages("tidylo",repos = "http://cran.us.r-project.org")
install.packages("RcppMeCab",repos = "http://cran.us.r-project.org")

library(tidylo)
library(tidyverse)
library(tidytext)
library(kableExtra)
library(wordcloud)
library(readxl)
```


```{r}
list.files("data/.")

list.files("data/knusenti/KnuSentiLex-master/")

senti_name_v <- list.files("data/knusenti/KnuSentiLex-master/.")[9]

senti_name_v

read_lines(str_c("data/knusenti/KnuSentiLex-master/", senti_name_v)) %>% head(10)

read_lines(str_c("data/knusenti/KnuSentiLex-master/", senti_name_v)) %>% 
  head(10) %>% str_extract("\t|\n| ")

read_tsv(str_c("data/knusenti/KnuSentiLex-master/", senti_name_v)) %>% head(10)
read_tsv(str_c("data/knusenti/KnuSentiLex-master/", senti_name_v), col_names = F) %>% head(10)

senti_dic_df <- read_tsv(str_c("data/knusenti/KnuSentiLex-master/", senti_name_v), col_names = F)

glimpse(senti_dic_df)

senti_dic_df[1-5, ]

senti_dic_df <- senti_dic_df %>% rename(word = X1, sScore = X2)
glimpse(senti_dic_df)

pkg_v <- c("tidyverse", "tidytext","RcppMeCab")

lapply(pkg_v, require, ch = T)

install.packages("stm", dependencies = T,repos = "http://cran.us.r-project.org")

install.packages("gt",repos = "http://cran.us.r-project.org")
```


```{r}

m_df <- readxl::read_excel("나이키.xlsx") %>% 
  select(제목, 본문)

s_df <- readxl::read_excel("아디다스.xlsx") %>% 
  select(제목, 본문)


```




```{r}

m_df2 <- m_df %>% 
  distinct(제목, .keep_all = T) %>% 
  mutate(ID = factor(row_number())) %>% 
  mutate(label = "0") %>%
  unite(제목, 본문, col = "text", sep = " ") %>% 
  mutate(text = str_squish(text))

m_tk <- m_df2 %>% 
  mutate(text = str_remove_all(text, "[^(\\w+|\\s)]")) %>%  
  unnest_tokens(word, text, token = "regex", drop = F) %>%
  count(word, sort = T)

m_tk <- 
m_tk %>% 
  filter(!word %in% c("나이키")) %>% 
  filter(str_detect(word, "[:alpha:]+")) %>%
  filter(str_length(word) > 1) %>%
  slice_max(n, n = 15) %>% 
  mutate(word = reorder(word, n))

m_tk %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(title = "나이키 단어 총빈도 분석")
```
### ★글로벌이 상위 주제어에 있었으나, 글로벌 브랜드라는 단어로 많이 사용되 삭제하기로 함 
### ★러시아가 상위어에 있는 이유는 러,우 전쟁으로 러시아에서 철수하게 되어 화제를 이끔
### ★애플사와 협업으로 애플워치 나이키모델이 나와 애플이 상위어에 나옴 
### ★아디다스에는 나이키가 상위어인데 반해 나이키는 아디다스가 상위어로 나오지 않음 


```{r}

s_df2 <- s_df %>% 
  distinct(제목, .keep_all = T) %>% 
  mutate(ID = factor(row_number())) %>% 
  mutate(label = "0") %>%
  unite(제목, 본문, col = "text", sep = " ") %>% 
  mutate(text = str_squish(text))

s_tk <- s_df2 %>% 
  mutate(text = str_remove_all(text, "[^(\\w+|\\s)]")) %>%  
  unnest_tokens(word, text, token = "regex", drop = F) %>%
  count(word, sort = T)

s_tk <- 
s_tk %>% 
  filter(!word %in% c("아디다스")) %>%   filter(str_detect(word, "[:alpha:]+")) %>%
  filter(str_length(word) > 1) %>%
  slice_max(n, n = 15) %>% 
  mutate(word = reorder(word, n))

s_tk %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(title = "아디다스 단어 총빈도 분석")
```

## 카타르 월드컵의 공인구나 협업 브랜드가 아디다스로 선정됨
### ★손흥민이 아디다스의 모델로 선정되어 상위어에 위치함 
### ★포토가 상위어에 있었으나, 포토존이나 여러 연예인들의 사진을 포토로 지정해 관련이 없다고 판단하여, 삭제함 
### ★가장 대적되는 기업인 나이키또한 아디다스 총빈도 분석에서 상위 6위에 들정도로 기사가 많이 남 



### 정리하면, 나이기, 아디다스에 있는 메타버스와 nft는 새로운 시장 개척지로 선정되어 
### 그와 관련된 중국의 유명 개발자가 있어 상위어에 중국이 나오게 되었고, 
### 아디다스는 카타르 월드컵 대표 브랜드로 선정되어 카타르 월드컵 관련 주제어가 많이나오는 반면
### 나이키는 상위어빈도가 크게 차이 나지않게 고루고루 분포되어 있음 
### 그리고 같은 기간내에 나이키의 기사수가 아디다스보다 약 500개정도 더 많았음 

```{r}

m_s_df <- m_df2 %>% 
  unnest_tokens(word, text, token = "regex") %>% 
  inner_join(senti_dic_df) %>% 
  count(word, sScore, sort = T) %>% 
  filter(!word %in% c("할인","세일","이벤트")) %>%
  filter(str_length(word) > 1) %>% 
  mutate(word = reorder(word, n)) %>% 
  slice_head(n = 20)

m_s_df %>% 
  ggplot() + geom_col(aes(n, word, fill = sScore), show.legend = F) +
    labs(title = "나이키감성분석")

```




```{r}

s_s_df <- s_df2 %>% 
  unnest_tokens(word, text, token = "regex") %>% 
  inner_join(senti_dic_df) %>% 
  count(word, sScore, sort = T) %>% 
  filter(!word %in% c("할인","세일")) %>%
  filter(str_length(word) > 1) %>% 
  mutate(word = reorder(word, n)) %>% 
  slice_head(n = 20)


s_s_df %>% 
  ggplot() + geom_col(aes(n, word, fill = sScore), show.legend = F) +
    labs(title = "아디다스 감성분석")


```

## 긍정 단어 중, 감사, 대상이 부정적인 의미로 쓰임에도 긍정적 단어로 분류되어 삭제하게 되었습니다. 
## 

```{r}

m_df2 %>% 
  unnest_tokens(word, text) %>% 
  left_join(senti_dic_df) %>% 
  mutate(sScore = ifelse(sScore >= 1, "긍정",
                         ifelse(sScore <= -1, "부정", "중립"))) 

m_df2 %>%   
  unnest_tokens(word, text, token = "regex") %>% 
  inner_join(senti_dic_df) %>% 
  mutate(emotion = ifelse(sScore > 0, "긍정", ifelse(sScore < 0, "부정", "중립"))) %>%
  mutate(label = ifelse(sScore > 0, "1", ifelse(sScore < 0, "0", "2"))) %>%
  filter(label != "중립") %>%
  filter(!word %in% c("할인","세일","이벤트","없다")) %>%
  count(word, emotion, label, sort = T) %>%
  filter(str_length(word) > 1) %>%
  group_by(label = ifelse(label > 0, "긍정", "부정")) %>%
  slice_head(n = 15) %>%
  ggplot(aes(x = n,
             y = reorder(word, n), fill = label)) +
  geom_col(show.legend = F) +
  facet_wrap(~label, scale = "free") +
  labs(title = "나이키 긍,부정어")
```
### ★ 나이키의 긍정어로 최고, 인기, 함께 등이 나왔고 
### ★ 부정어로는 살인, 눈물, 위기등이 나온 이유는 나이키 ceo의 과거 살인 행적 고백때문에 나온 것입니다. 

```{r}

s_df2 %>% 
  unnest_tokens(word, text) %>% 
  filter(!word %in% c("피해")) %>%
  left_join(senti_dic_df) %>% 
  mutate(sScore = ifelse(sScore >= 1, "긍정",
                         ifelse(sScore <= -1, "부정", "중립"))) 

s_df2 %>%   
  unnest_tokens(word, text, token = "regex") %>% 
  inner_join(senti_dic_df) %>% 
  mutate(emotion = ifelse(sScore > 0, "긍정", ifelse(sScore < 0, "부정", "중립"))) %>%
  mutate(label = ifelse(sScore > 0, "1", ifelse(sScore < 0, "0", "2"))) %>%
  filter(label != "중립") %>%
  filter(!word %in% c("할인","세일","이벤트")) %>%
  count(word, emotion, label, sort = T) %>%
  filter(str_length(word) > 1) %>%
  filter(!word %in% c("한국수자원공사", "수자원공사", "환경부")) %>%
  group_by(label = ifelse(label > 0, "긍정", "부정")) %>%
  slice_head(n = 15) %>%
  ggplot(aes(x = n,
             y = reorder(word, n), fill = label)) +
  geom_col(show.legend = F) +
  facet_wrap(~label, scale = "free") +
  labs(title = "아디다스 긍,부정어")
```
### ★ 아디다스의 긍정어로 최고, 함께, 수익등이 나이키와 같이 나왔고 
### ★ 부정어로 혐오, 굴욕 등이 나온 이유는 아디다스 모델인 칸예 웨스트의 혐오발언으로 계약을  해지해서 입니다. 
### ★ 나이키와의 기사수의 차이가 있지만,  긍정어의 개수는 비슷하고 부정어의 개수는 낮은 것을 보아 긍정적인 기사수가 좀 더 높은 것으로 파악됩니다. 



```{r}

weighted_log_odds_df1 <-
  bind_rows(m_tk, s_tk, .id = "party") %>% 
  bind_log_odds(set = party,
                feature = word, 
                n = n) %>% 
  arrange(-log_odds_weighted)
```

```{r}

library(gt)
library(dplyr)

m.s_df <- bind_cols(
  weighted_log_odds_df1 %>%   
  group_by(party = ifelse(party == 1, "나이키", "아디다스")) %>% 
  arrange(party) %>% 
  select(-party) %>%
  head(15),
  
  weighted_log_odds_df1 %>%   
  group_by(party = ifelse(party == 1, "나이키", "아디다스")) %>% 
  arrange(desc(party)) %>% 
  select(-party) %>%
  head(15) 
  ) 

m.s_df <- m.s_df[-c(1,5)]


m.s_df %>%
  gt() %>% tab_header(
  "상대 빈도 분석"
  ) %>% tab_spanner(
    label = "나이키 기준",
    columns = 1:3
  ) %>% tab_spanner(
    label = "아디다스 기준",
    columns = 4:6
  ) %>% cols_label(
    word...2 = "명사",
    n...3 = "빈도",
    log_odds_weighted...4 = "가중상대빈도",
    word...6 = "명사",
    n...7 = "빈도",
    log_odds_weighted...8 = "가중상대빈도"
  ) %>% fmt_number(
    columns = starts_with("log"), 
    decimals = 2
  )
```

```{r}

s_ttk <- s_df2 %>% 
  mutate(text = str_remove_all(text, "[^(\\w+|\\s)]")) %>%  
  unnest_tokens(word, text, token = "regex", drop = F)

s_ttk <- 
s_ttk %>%
  filter(str_detect(word, "[:alpha:]+"))

s_cdf <- s_ttk %>%
  group_by(ID) %>%
  summarise(text2 = str_flatten(word, " ")) %>%
  ungroup() %>% 
  inner_join(s_df2, by = "ID")

library(stm)
library(tm)

processed <- s_df2 %>% 
  textProcessor(
    documents = s_cdf$text2,
    metadata = .,
    wordLengths = c(2, Inf))

```

```{r}

out <- 
  prepDocuments(processed$documents,
                     processed$vocab,
                     processed$meta,
                 lower.thresh = 0)

```

```{r}

docs <- out$documents
vocab <- out$vocab
meta <- out$meta

```

```{r}

topicN <- c(3, 7)



```


```{r}

s_stm_fit <-
 stm(
    documents = docs,
    vocab = vocab,
    K = 6,
    data = meta,
    max.em.its = 75,
    init.type = "Spectral",
    seed = 25,
    verbose = F
  )

```


```{r}

   
s_topic_name <- tibble(topic = 1:6,
                     name = c("1. 협업 및 판매",
                              "2. 카타르 월드컵",
                              "3. 할인 행사",
                              "4. 아이돌 계약",
                              "5.계약해지 논란",
                              "6. 온라인 쇼핑몰"))

s_td_beta <- s_stm_fit %>% tidy(matrix = 'beta')

s_topic_name <- s_td_beta %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 7) %>% 
  left_join(s_topic_name, by = "topic")

s_topic_name %>% 
  ggplot(aes(x = beta, 
             y = reorder_within(term, beta, name),  
             fill = name)) +
  geom_col(show.legend = F) +
  facet_wrap(~name, scales = "free") +
  scale_y_reordered() +                             
  labs(x = expression("단어 확률분포: "~beta), y = NULL,
       title = "수자원 주제별 단어 확률 분포") +
  theme(plot.title = element_text(size = 20))


```


```{r} 

s_td_gamma <- s_stm_fit %>% tidy(matrix = "gamma") 
s_top_terms <- 
s_td_beta %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 5) %>% 
  select(topic, term) %>% 
  summarise(terms = str_flatten(term, collapse = ", ")) 

s_gamma_terms <- 
s_td_gamma %>% 
  group_by(topic) %>% 
  summarise(gamma = mean(gamma)) %>% 
  left_join(s_top_terms, by = 'topic') %>% 
  left_join(s_topic_name, by = 'topic')
  
s_gamma_terms %>% 
  
  ggplot(aes(x = gamma, y = reorder(name, gamma), fill = name)) +
  geom_col(show.legend = F) +
  geom_text(aes(label = round(gamma, 2)), 
            hjust = 1.15) +                
  geom_text(aes(label = terms), 
            hjust = -0.05) +              
  labs(x = expression("문서 확률분포"~(gamma)), y = NULL,
       title = "아디다스 토픽 상위 주제어") +
  theme(plot.title = element_text(size = 20))

```

```{r}

m_ttk <- m_df2 %>% 
  mutate(text = str_remove_all(text, "[^(\\w+|\\s)]")) %>%  
  unnest_tokens(word, text, token = "regex", drop = F)

m_ttk <- 
m_ttk %>%
  filter(str_detect(word, "[:alpha:]+"))

m_cdf <- m_ttk %>%
  group_by(ID) %>%
  summarise(text2 = str_flatten(word, " ")) %>%
  ungroup() %>% 
  inner_join(m_df2, by = "ID")

library(stm)
library(tm)

processed <- m_df2 %>% 
  textProcessor(
    documents = m_cdf$text2,
    metadata = .,
    wordLengths = c(2, Inf))

```

```{r}

out <- 
  prepDocuments(processed$documents,
                     processed$vocab,
                     processed$meta,
                 lower.thresh = 0)

```

```{r}

docs <- out$documents
vocab <- out$vocab
meta <- out$meta

```

```{r}

topicN <- c(3, 7)



```


```{r}

m_stm_fit <-
 stm(
    documents = docs,
    vocab = vocab,
    K = 6,
    data = meta,
    max.em.its = 75,
    init.type = "Spectral",
    seed = 25,
    verbose = F
  )

```


```{r}

   
m_topic_name <- tibble(topic = 1:6,
                     name = c("1. 협업 및 판매",
                              "2. 카타르 월드컵",
                              "3. 할인 행사",
                              "4. 아이돌 계약",
                              "5.계약해지 논란",
                              "6. 온라인 쇼핑몰"))

m_td_beta <- m_stm_fit %>% tidy(matrix = 'beta')

m_topic_name <- m_td_beta %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 7) %>% 
  left_join(m_topic_name, by = "topic")

m_topic_name %>% 
  ggplot(aes(x = beta, 
             y = reorder_within(term, beta, name),  
             fill = name)) +
  geom_col(show.legend = F) +
  facet_wrap(~name, scales = "free") +
  scale_y_reordered() +                             
  labs(x = expression("단어 확률분포: "~beta), y = NULL,
       title ="나이키 주제별 단어 확률 분포") +
  theme(plot.title = element_text(size = 20))


```


```{r} 

m_td_gamma <- m_stm_fit %>% tidy(matrix = "gamma") 
m_top_terms <- 
m_td_beta %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 5) %>% 
  select(topic, term) %>% 
  summarise(terms = str_flatten(term, collapse = ", ")) 

m_gamma_terms <- 
m_td_gamma %>% 
  group_by(topic) %>% 
  summarise(gamma = mean(gamma)) %>% 
  left_join(m_top_terms, by = 'topic') %>% 
  left_join(m_topic_name, by = 'topic')
  
m_gamma_terms %>% 
  ggplot(aes(x = gamma, y = reorder(name, gamma), fill = name)) +
  geom_col(show.legend = F) +
  geom_text(aes(label = round(gamma, 2)), 
            hjust = 1.15) +                
  geom_text(aes(label = terms), 
            hjust = -0.05) +              
  labs(x = expression("문서 확률분포"~(gamma)), y = NULL,
       title = "나이키 토픽 상위 주제어") +
  theme(plot.title = element_text(size = 20))

```